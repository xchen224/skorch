{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+02 8.50350014e-04 5.00000024e-04 1.29142904e+00] [[ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]\n",
      " [ 8.80761984e-03 -2.30353642e-06]] [[ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]\n",
      " [ 0.99967602 -0.67754029]] [1.29571462 1.30000031 1.304286   1.3085717  1.31285739 1.31714296\n",
      " 1.32142866 1.32571435] [100.          -7.06986251  -7.60090241   1.29571462] (101360, 1)\n",
      "101360 (101360, 3)\n",
      "[100.          -7.06986251  -7.60090241   1.29571462] [[-12.33271642]\n",
      " [-12.33271642]\n",
      " [-12.33271642]\n",
      " ...\n",
      " [ -6.31135514]\n",
      " [ -6.31135514]\n",
      " [ -6.31135514]] [[-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]\n",
      " [-6.31135514e+00  9.33063080e-03 -7.68182258e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "task = 0\n",
    "mlp_layers = 4\n",
    "mlp_dim = 768 #768\n",
    "max_epochs = 2000\n",
    "lr = 0.01\n",
    "min_lr = 0.0001\n",
    "\n",
    "# T-matrix data\n",
    "filename = '/Dedicated/jwang-data/xchen/non-spherical/NN/py/results/tmat_fine_n_intg.pickle'\n",
    "with open(filename, 'rb') as f:\n",
    "    TMdata = pickle.load(f)\n",
    "X_ori = np.reshape(np.array(TMdata['x']),(int(len(TMdata['x'])/4),4))\n",
    "y_ori = np.log(TMdata['value']['ext'])\n",
    "xk    = X_ori[:,2]\n",
    "xn    = X_ori[:,3]\n",
    "J_ori = np.reshape( np.append( TMdata['Jacmi']['ext'] / X_ori[:,2], TMdata['Jacmr']['ext'] ), (2,len(TMdata['Jacmr']['ext'])) )\n",
    "idx   = np.where( (np.abs(X_ori[:,-1]-1.291429) > 1.0e-6 ) & (np.abs(X_ori[:,-1]-1.33) > 1.0e-6 ) )[0]\n",
    "X1    = X_ori[idx,:]\n",
    "X1[:,1] = np.log(X1[:,1])\n",
    "X1[:,2] = np.log(X1[:,2])\n",
    "y1    = y_ori[idx].reshape(-1,1)\n",
    "J1    = J_ori[:,idx].T\n",
    "J2    = np.empty(J1.shape)\n",
    "J2[:,0]  = J1[:,0] / np.exp(y1[:,0]) * xk[idx]\n",
    "J2[:,1]  = J1[:,1] / np.exp(y1[:,0]) * xn[idx]\n",
    "\n",
    "print( X_ori[100,:], J1[:10,:], J2[:10,:], X1[::181*14*5,-1], X1[100,:], y1.shape )\n",
    "\n",
    "# combine\n",
    "J_mean = J2.mean(axis=0, keepdims=True)\n",
    "J_std  = J2.std(axis=0, keepdims=True)\n",
    "norm_J = (J2 - J_mean) / J_std\n",
    "num_tasks = 1\n",
    "allX  = X1\n",
    "ally  = y1\n",
    "allJ  = J1 #norm_J\n",
    "UseX  = allX\n",
    "num_examples, num_features = UseX.shape\n",
    "Usey  = np.hstack((ally,allJ))\n",
    "    \n",
    "#\n",
    "#\n",
    "print(num_examples, Usey.shape)\n",
    "print(X1[100,:],y1, Usey[-10:,:])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996978488791617 0.9996991813552724\n"
     ]
    }
   ],
   "source": [
    "# Split dataset\n",
    "train_x, test_x, train_y, test_y = train_test_split(UseX, Usey, test_size=0.2)\n",
    "    \n",
    "x_mean = train_x.mean(axis=0, keepdims=True)\n",
    "x_std = train_x.std(axis=0, keepdims=True)\n",
    "train_x = (train_x - x_mean) / x_std\n",
    "test_x = (test_x - x_mean) / x_std\n",
    "    \n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = torch.Tensor(train_y)\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = torch.Tensor(test_y)\n",
    "    \n",
    "# Transform data\n",
    "x_mean = torch.Tensor(x_mean).cuda()\n",
    "x_std = torch.Tensor(x_std).cuda()\n",
    "J_mean = torch.Tensor(J_mean).cuda()\n",
    "J_std = torch.Tensor(J_std).cuda()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()\n",
    "\n",
    "for sf in [1.0]: #[0.1, 0.01, 0.001]:\n",
    "        \n",
    "    # Construct Pytorch/Skorch model\n",
    "    class AttrProxy(object):\n",
    "        \"\"\"Translates index lookups into attribute lookups.\"\"\"\n",
    "        def __init__(self, module, prefix):\n",
    "            self.module = module\n",
    "            self.prefix = prefix\n",
    "    \n",
    "        def __getitem__(self, i):\n",
    "            return getattr(self.module, self.prefix + str(i))\n",
    "  \n",
    "    class Net(torch.nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "    \n",
    "            input_dim = num_features\n",
    "            for i in range(mlp_layers-1):\n",
    "                self.add_module('fc_' + str(i), nn.Linear(input_dim, mlp_dim))\n",
    "                input_dim = mlp_dim\n",
    "            self.fc = AttrProxy(self, 'fc_')\n",
    "            self.fc1 = nn.Linear(input_dim, num_tasks)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            for i in range(mlp_layers-1):\n",
    "                fc = self.fc.__getitem__(i)\n",
    "                x = torch.tanh(fc(x))\n",
    "            x = self.fc1(x)\n",
    "            return x\n",
    "\n",
    "    from skorch.utils import TeeGenerator\n",
    "    class MyNeuralNetRegressor(NeuralNetRegressor):\n",
    "        def train_step_single(self, Xi, yi, **fit_params):\n",
    "            Xi = Xi.requires_grad_(True)\n",
    "            batch_size = Xi.shape[0]\n",
    "            \n",
    "            # split yi into values and jacobians\n",
    "            y0  = torch.reshape(yi[:,0],(len(yi[:,0]),1))\n",
    "            self.module_.train()\n",
    "            y_pred = self.infer(Xi, **fit_params)\n",
    "            #print('y_pred',y_pred)\n",
    "            #print('y0',y0)\n",
    "        \n",
    "            y_pred.backward(torch.eye(batch_size,).to(device), retain_graph=True)\n",
    "            \n",
    "            # gradient calculation\n",
    "            self.module_.zero_grad()\n",
    "            grad = Xi.grad[:,2:]\n",
    "            grad_ref = yi[:,1:].requires_grad_(True)       \n",
    "            grad0 = grad[:,0] / x_std[0][2] \n",
    "            grad1 = grad[:,1] / x_std[0][3]\n",
    "            normgrad0 = (grad0 - J_mean[0][0]) / J_std[0][0]\n",
    "            normgrad1 = (grad1 - J_mean[0][1]) / J_std[0][1]\n",
    "            \n",
    "            # gradient loss\n",
    "            #print('J1_pred',J1_pred, grad_ref[:,1] * sample_weight)\n",
    "            grad_loss = self.get_loss(normgrad0, grad_ref[:,0], X=Xi, training=True) + \\\n",
    "                        self.get_loss(normgrad1, grad_ref[:,1], X=Xi, training=True)\n",
    "            #print('grad_loss',grad_loss)\n",
    "            #loss = self.get_loss(y_pred, y0, X=Xi, training=True) + \\\n",
    "            #                s1 * delta_grad0 + s2 * delta_grad1\n",
    "            \n",
    "            loss = self.get_loss(y_pred, y0, X=Xi, training=True)  + grad_loss\n",
    "                            \n",
    "            loss.backward()\n",
    "            self.notify(\n",
    "                    'on_grad_computed',\n",
    "                    named_parameters=TeeGenerator(self.module_.named_parameters()),\n",
    "                    X=Xi,\n",
    "                    y=y0\n",
    "                )\n",
    "\n",
    "            return {\n",
    "                    'loss': loss,\n",
    "                    'y_pred': y_pred,\n",
    "                    }\n",
    "    \n",
    "    \n",
    "        def validation_step(self, Xi, yi, **fit_params):\n",
    "            \"\"\"Perform a forward step using batched data and return the\n",
    "                resulting loss.\n",
    "                The module is set to be in evaluation mode (e.g. dropout is\n",
    "                not applied).\n",
    "                Parameters\n",
    "                ----------\n",
    "                Xi : input data\n",
    "                  A batch of the input data.\n",
    "                yi : target data\n",
    "                  A batch of the target data.\n",
    "                **fit_params : dict\n",
    "                  Additional parameters passed to the ``forward`` method of\n",
    "                  the module and to the ``self.train_split`` call.\n",
    "            \"\"\"\n",
    "                \n",
    "            # split yi\n",
    "            y0  = torch.reshape(yi[:,0],(len(yi[:,0]),1))\n",
    "            Xi = Xi.requires_grad_(True)\n",
    "            batch_size = Xi.shape[0]\n",
    "                \n",
    "            self.module_.eval()\n",
    "            y_pred = self.infer(Xi, **fit_params)\n",
    "            y_pred.backward(torch.eye(batch_size,).to(device), retain_graph=True)\n",
    "                \n",
    "            grad = Xi.grad[:,2:]\n",
    "            grad_ref = yi[:,1:]\n",
    "                \n",
    "            grad0 = grad[:,0] / x_std[0][2] \n",
    "            grad1 = grad[:,1] / x_std[0][3]\n",
    "            normgrad0 = (grad0 - J_mean[0][0]) / J_std[0][0]\n",
    "            normgrad1 = (grad1 - J_mean[0][1]) / J_std[0][1]\n",
    "            \n",
    "            grad_loss = self.get_loss(normgrad0, grad_ref[:,0], X=Xi, training=False) + \\\n",
    "                        self.get_loss(normgrad1, grad_ref[:,1], X=Xi, training=False)\n",
    "            #print('valid grad:', grad)\n",
    "                \n",
    "            yloss = self.get_loss(y_pred, y0, X=Xi, training=False)\n",
    "            loss = yloss + grad_loss\n",
    "            y_loss.append( yloss.item() )\n",
    "            allgrad_loss.append( grad_loss.item() )\n",
    "\n",
    "            #print('valid_loss:', grad_loss, yloss)\n",
    "\n",
    "#             y_loss.append(yloss.item())\n",
    "#             grad0_loss.append( s1 * delta_grad0.item() )\n",
    "#             grad1_loss.append( s2 * delta_grad1.item() )\n",
    "                \n",
    "            return {\n",
    "                    'loss': loss,\n",
    "                    'y_pred': y_pred,\n",
    "                    }\n",
    "\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    net_regr = MyNeuralNetRegressor(\n",
    "            Net,\n",
    "            max_epochs=max_epochs,\n",
    "            batch_size=1024,\n",
    "            lr=lr,\n",
    "            device=device,\n",
    "    )\n",
    "\n",
    "    from skorch.callbacks import Checkpoint, LoadInitState, LRScheduler\n",
    "    cp1 = Checkpoint(dirname='./tanh_logX_n_add_oriJ_10sf/')  #tanh_logX_n_add_lnJ\n",
    "\n",
    "    net_regr.initialize()\n",
    "    net_regr.load_params(checkpoint=cp1)\n",
    "\n",
    "    train_score = net_regr.score(train_x.cpu(), train_y[:,0].cpu())\n",
    "    test_score = net_regr.score(test_x.cpu(), test_y[:,0].cpu())\n",
    "    print(train_score, test_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.480963e-04 8.503500e-04 1.115722e-03 1.463909e-03 1.920756e-03\n",
      " 2.520173e-03 3.306652e-03 4.338570e-03 5.692523e-03 7.469008e-03\n",
      " 9.799887e-03 1.285817e-02 1.687087e-02 2.213582e-02 2.904382e-02\n",
      " 3.810762e-02 5.000000e-02 6.560367e-02 8.607684e-02 1.129391e-01\n",
      " 1.481844e-01 1.944289e-01 2.551050e-01 3.347165e-01 4.391726e-01\n",
      " 5.762267e-01 7.560518e-01 9.919955e-01 1.301571e+00 1.707757e+00\n",
      " 2.240702e+00 2.939966e+00 3.857452e+00 5.061260e+00 6.640745e+00\n",
      " 8.713145e+00 1.143229e+01 1.500000e+01 1.968110e+01 2.582305e+01\n",
      " 3.388174e+01] [0.00085035 0.00111572 0.00146391 0.00192076 0.00252017 0.00330665\n",
      " 0.00433857 0.00569252 0.00746901 0.00979989 0.01285817 0.01687087\n",
      " 0.02213582 0.02904382] [0.0005     0.00081895 0.00134135 0.00219698 0.00359843 0.00589384\n",
      " 0.00965349 0.01581139 0.02589737 0.04241714 0.06947477 0.1137923\n",
      " 0.1863797  0.3052701  0.5       ] [0.0005     0.00219699 0.00965349 0.04241714 0.1863797 ] [1.291429 1.310714 1.33     1.349286 1.368571 1.387857 1.407143 1.426429\n",
      " 1.445714 1.465    1.484286 1.503571 1.522857 1.542143 1.561429 1.580714\n",
      " 1.6      1.619286 1.638571 1.657857 1.677143 1.696429] [1.29571462 1.30000031 1.304286   1.3085717  1.31285739 1.31714296\n",
      " 1.32142866 1.32571435]\n"
     ]
    }
   ],
   "source": [
    "rr0 = np.sort( np.array( list(set(list(np.exp(X0[:,1])))) ) )\n",
    "rr1 = np.sort( np.array( list(set(list(np.exp(X1[:,1])))) ) )\n",
    "kk0 = np.sort( np.array( list(set(list(np.exp(X0[:,2])))) ) )\n",
    "kk1 = np.sort( np.array( list(set(list(np.exp(X1[:,2])))) ) )\n",
    "nn0 = np.sort( np.array( list(set(list(X0[:,3]))) ) )\n",
    "nn1 = np.sort( np.array( list(set(list(X1[:,3]))) ) )\n",
    "print(rr0, rr1, kk0, kk1, nn0, nn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(836220, 4)\n",
      "cho_idx 101360\n"
     ]
    }
   ],
   "source": [
    "# Dubovik's database\n",
    "X0 = np.load(\"/Dedicated/jwang-data/xchen/non-spherical/NN/py/results/X_untrans.npy\")\n",
    "X0[:,1] = np.log(X0[:,1])\n",
    "X0[:,2] = np.log(X0[:,2])\n",
    "y0 = np.log(np.load(\"/Dedicated/jwang-data/xchen/non-spherical/NN/py/results/y_untrans.npy\")[:, task:task+1])\n",
    "\n",
    "\n",
    "# read Tmatrix data\n",
    "#X0   = X\n",
    "flag = np.logical_and(X0[:,1] > np.log(0.0008),X0[:,1] < np.log(0.03))\n",
    "idx0 = np.where(flag == True)[0]\n",
    "pred_X = X0[idx0,:]\n",
    "print(pred_X.shape)\n",
    "\n",
    "tmaty = {}\n",
    "tmatJ = {}\n",
    "with open('/Dedicated/jwang-data/xchen/non-spherical/NN/py/results/tmat_Jacall_norm_intg.pickle', 'rb') as handle:\n",
    "    data2 = pickle.load(handle)\n",
    "for key in data2['value'].keys():\n",
    "    tmaty[key] = data2['value'][key]\n",
    "    tmatJ[key] = np.reshape( np.append( data2['Jacmi'][key], data2['Jacmr'][key] ), (2,len(data2['value'][key])) )\n",
    "cho_idx = data2['idx']\n",
    "print('cho_idx',len(cho_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 4) [ 0.         -7.06986253 -7.53168647  1.291429  ] [[ 0.         -7.06986253 -7.60090246  1.291429  ]\n",
      " [ 0.         -7.06986253 -6.12066931  1.291429  ]\n",
      " [ 0.         -7.06986253 -4.64043587  1.291429  ]\n",
      " [ 0.         -7.06986253 -3.16020275  1.291429  ]\n",
      " [ 0.         -7.06986253 -1.67996929  1.291429  ]]\n",
      "jac: (500, 4)\n",
      "[[5.2266026e+01 1.0952133e+00 2.0918984e+00 9.8287677e-03]]\n",
      "(1000, 4)\n",
      "jac: (1000, 4)\n",
      "[[5.2266026e+01 1.0952133e+00 2.0918984e+00 9.8287677e-03]]\n",
      "(1000, 4) [ 0.         -7.06986253 -7.60090246  1.29345603] [[ 0.         -7.06986253 -7.60090246  1.291429  ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.349286  ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.407143  ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.465     ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.522857  ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.580714  ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.638571  ]\n",
      " [ 0.         -7.06986253 -7.60090246  1.696429  ]] [[ 0.         -7.06986251 -7.60090241  1.29571462]\n",
      " [ 0.         -7.06986251 -7.60090241  1.30000031]\n",
      " [ 0.         -7.06986251 -7.60090241  1.304286  ]\n",
      " [ 0.         -7.06986251 -7.60090241  1.3085717 ]\n",
      " [ 0.         -7.06986251 -7.60090241  1.31285739]\n",
      " [ 0.         -7.06986251 -7.60090241  1.31714296]\n",
      " [ 0.         -7.06986251 -7.60090241  1.32142866]\n",
      " [ 0.         -7.06986251 -7.60090241  1.32571435]]\n",
      "jac: (1000, 4)\n",
      "[[5.2266026e+01 1.0952133e+00 2.0918984e+00 9.8287677e-03]]\n"
     ]
    }
   ],
   "source": [
    "Xa = X0\n",
    "y0 = y\n",
    "def get_ut_jacobian(net, x):\n",
    "    batch_size = x.shape[0]\n",
    "    xt = x \n",
    "    xt.requires_grad_(True)\n",
    "    y = torch.exp(net(xt))\n",
    "    #print(y.detach().cpu().numpy().shape)\n",
    "    y.backward(torch.eye(batch_size).cuda())\n",
    "    return y.detach().cpu().numpy(), xt.grad.detach().cpu().numpy(), xt.detach().cpu().numpy()\n",
    "\n",
    "def get_jacobian(net, x):\n",
    "    batch_size = x.shape[0]\n",
    "    x.requires_grad_(True)\n",
    "    y = net(x)\n",
    "    y.backward(torch.eye(batch_size).cuda())\n",
    "    return y.detach().cpu().numpy(), x.grad.detach().cpu().numpy()\n",
    "\n",
    "def make_plot_lnx2(n=1):\n",
    "    \n",
    "    \n",
    "    x2 = np.linspace(Xa[0, 2], Xa[(n-1)*41*181, 2], 500)\n",
    "    x0 = np.ones_like(x2)*Xa[0, 0]\n",
    "    x1 = np.ones_like(x2)*Xa[182, 1]\n",
    "    x3 = np.ones_like(x2)*Xa[0, 3]\n",
    "    Xp = np.stack([x0, x1, x2, x3], axis=1)\n",
    "    print(Xp.shape, Xp[5,:],pred_X[cho_idx,:][0:181*14*5:181*14,:])\n",
    "    Xp = torch.Tensor(Xp).cuda()\n",
    "    Xp = (Xp-x_mean)/x_std\n",
    "    yp, jac, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "    print('jac:',jac.shape)\n",
    "    #-x_mean.cpu().numpy())/x_std.cpu().numpy()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    Xp = Xp.detach().cpu().numpy()\n",
    "    print(x_std.cpu().numpy())\n",
    "    ax1.scatter(x2, jac[:, 2]/x_std.cpu().numpy()[0][2], s=1, c='C0', alpha=0.7)\n",
    "    ax1.scatter(pred_X[cho_idx,2][0:181*14*5:181*14], tmatJ['ext'][0,0:181*14*5:181*14], s=5, c='C3', alpha=0.7 )\n",
    "    ax1.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{lnk}}$', color='C0')\n",
    "    ax1.tick_params(axis='y', labelcolor='C0')\n",
    "    ax1.set_xlabel('lnk')\n",
    "    ax1.legend(['NN-Jac','Tmat-Jac'],loc='upper left')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x2, yp, s=1, c='C1', alpha=0.7, label='predict')\n",
    "    for ip in range(n):\n",
    "        xl = Xa[181*41*ip,:] \n",
    "        if ip == 0:\n",
    "            ax2.scatter(xl[2], np.exp(y0[181*41*ip]), s=5, c='C2', alpha=0.7, label='true')\n",
    "        else:\n",
    "            ax2.scatter(xl[2], np.exp(y0[181*41*ip]), s=5, c='C2', alpha=0.7)\n",
    "    ax2.scatter(pred_X[cho_idx,2][0:181*14*5:181*14], tmaty['ext'][0:181*14*5:181*14], s=5, c='C4', alpha=0.7, label='Tmat')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_ylabel('cext', color='C1')\n",
    "    ax2.tick_params(axis='y', labelcolor='C1')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('img/jac_logX_lnk_tanh_base_lnJ_fine_n.png', dpi=300, facecolor='w', edgecolor='w')\n",
    "    plt.close()\n",
    "    \n",
    "def make_plot_x2(n=1):\n",
    "    \n",
    "    \n",
    "    x2 = np.linspace(Xa[0,2], Xa[(n-1)*41*181, 2], 1000)\n",
    "    x0 = np.ones_like(x2)*Xa[0, 0]\n",
    "    x1 = np.ones_like(x2)*Xa[182, 1]\n",
    "    x3 = np.ones_like(x2)*Xa[0, 3]\n",
    "    Xp = np.stack([x0, x1, x2, x3], axis=1)\n",
    "    print(Xp.shape)\n",
    "    Xp = torch.Tensor(Xp).cuda()\n",
    "    Xp = (Xp-x_mean)/x_std\n",
    "    yp, jac, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "    print('jac:',jac.shape)\n",
    "    #-x_mean.cpu().numpy())/x_std.cpu().numpy()\n",
    "    Xp = Xp.detach().cpu().numpy()\n",
    "    print(x_std.cpu().numpy())\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.scatter(x2, jac[:, 2]/x_std.cpu().numpy()[0][2]/np.exp(x2), s=1, c='C0', alpha=0.7)\n",
    "    tJ  = tmatJ['ext'][0,:]/np.exp(pred_X[cho_idx,2])\n",
    "    ax1.scatter(pred_X[cho_idx,2][0:181*14*5:181*14], tJ[0:181*14*5:181*14], s=5, c='C3', alpha=0.7 )\n",
    "    ax1.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$', color='C0')\n",
    "    ax1.tick_params(axis='y', labelcolor='C0')\n",
    "    ax1.set_xlabel('k')\n",
    "    ax1.legend(['NN-Jac','Tmat-Jac'],loc='upper left')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x2, yp, s=1, c='C1', alpha=0.7, label='predict')\n",
    "    for ip in range(n):\n",
    "        xl = Xa[181*41*ip,:] \n",
    "        if ip == 0:\n",
    "            ax2.scatter(xl[2], np.exp(y0[181*41*ip]), s=5, c='C2', alpha=0.7, label='true')\n",
    "        else:\n",
    "            ax2.scatter(xl[2], np.exp(y0[181*41*ip]), s=5, c='C2', alpha=0.7)\n",
    "    ax2.scatter(pred_X[cho_idx,2][0:181*14*5:181*14], tmaty['ext'][0:181*14*5:181*14], s=5, c='C4', alpha=0.7, label='Tmat')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_ylabel('cext', color='C1')\n",
    "    ax2.tick_params(axis='y', labelcolor='C1')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('img/jac_logX_k_tanh_base_lnJ_fine_n.png', dpi=300, facecolor='w', edgecolor='w')\n",
    "    plt.close()\n",
    "    \n",
    "def make_plot_x3(n=1):\n",
    "    \n",
    "    x3 = np.linspace(Xa[0, 3], Xa[(n-1)*41*181*15, 3], 1000)\n",
    "    x2 = np.ones_like(x3)*Xa[0, 2]\n",
    "    x0 = np.ones_like(x3)*Xa[0, 0]\n",
    "    x1 = np.ones_like(x3)*Xa[182, 1]\n",
    "    Xp = np.stack([x0, x1, x2, x3], axis=1)\n",
    "    print(Xp.shape, Xp[5,:], pred_X[cho_idx,:][::181*14*5,:], X1[::181*14*5,:])\n",
    "    Xp = torch.Tensor(Xp).cuda()\n",
    "    Xp = (Xp-x_mean)/x_std\n",
    "    yp, jac, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "    print('jac:',jac.shape)\n",
    "    Xp = Xp.detach().cpu().numpy()\n",
    "    print(x_std.cpu().numpy())\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.scatter(x3, jac[:, 3]/x_std.cpu().numpy()[0][3], s=1, c='C0', alpha=0.7)\n",
    "    ax1.scatter(pred_X[cho_idx,3][::181*14*5], tmatJ['ext'][1,::181*14*5], s=5, c='C3', alpha=0.7 )\n",
    "    ax1.scatter(X1[::181*14*5,3], J1[::181*14*5,1], s=5, c='C3', alpha=0.7 )\n",
    "    ax1.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{n}}$', color='C0')\n",
    "    ax1.tick_params(axis='y', labelcolor='C0')\n",
    "    ax1.set_xlabel('n')\n",
    "    ax1.legend(['NN-Jac','Tmat-Jac'],loc='upper right')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.scatter(x3, yp, s=1, c='C1', alpha=0.7, label='predict')\n",
    "    for ip in range(n):\n",
    "        xl = Xa[181*41*15*ip,3] \n",
    "        if ip == 0:\n",
    "            ax2.scatter(xl, np.exp(y0[181*41*15*ip]), s=5, c='C2', alpha=0.7,label='true')\n",
    "        else:\n",
    "            ax2.scatter(xl, np.exp(y0[181*41*15*ip]), s=5, c='C2', alpha=0.7)\n",
    "    ax2.scatter(pred_X[cho_idx,3][::181*14*5], tmaty['ext'][::181*14*5], s=5, c='C4', alpha=0.7, label='Tmat')\n",
    "    ax2.scatter(X1[::181*14*5,3], np.exp(y1[::181*14*5,0]), s=5, c='C4', alpha=0.7)\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.set_ylabel('cext', color='C1')\n",
    "    ax2.tick_params(axis='y', labelcolor='C1')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('img/jac_logX_n_tanh_base_lnJ_fine_n.png', dpi=300, facecolor='w', edgecolor='w')\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    \n",
    "def make_ut_plot(n=1):\n",
    "    \n",
    "    x0 = np.linspace(X[41*(n-1), 0], X[41*(n)-1, 0], 500)\n",
    "    x1 = np.ones_like(x0)*X[41*(n-1), 1]\n",
    "    x2 = np.ones_like(x0)*X[41*(n-1), 2]\n",
    "    Xp = np.stack([x0, x1, x2], axis=1)\n",
    "    print(Xp.shape)\n",
    "    Xp = torch.Tensor(Xp).cuda()\n",
    "\n",
    "    yp, jac, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "    xl = (X[41*(n-1):41*n,:]-x_mean.cpu().numpy())/x_std.cpu().numpy()\n",
    "    plt.scatter(xt[:, 0], jac[:, 0])\n",
    "    plt.scatter(xt[:, 0], yp)\n",
    "    plt.scatter(xl[:, 0], np.exp(y[41*(n-1):41*n]))\n",
    "    plt.legend(['jac', 'y', 'target'])\n",
    "    plt.savefig('jac_ut.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "#make_ut_plot(n=1)\n",
    "make_plot_lnx2(n=15)\n",
    "\n",
    "#fig = plt.figure()\n",
    "make_plot_x2(n=15)\n",
    "\n",
    "make_plot_x3(n=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = torch.exp(net_regr.module_(train_x)).cpu().numpy()\n",
    "targets = torch.exp(train_y).cpu().numpy()\n",
    "print(r2_score(targets, preds))\n",
    "print(mean_absolute_error(targets, preds))\n",
    "print(mean_squared_error(targets, preds))\n",
    "print(mean_absolute_percentage_error(targets, preds))\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = torch.exp(net_regr.module_(test_x)).cpu().numpy()\n",
    "targets = torch.exp(test_y).cpu().numpy()\n",
    "print(r2_score(targets, preds))\n",
    "print(mean_absolute_error(targets, preds))\n",
    "print(mean_squared_error(targets, preds))\n",
    "print(mean_absolute_percentage_error(targets, preds))\n",
    "\n",
    "plt.scatter(targets, preds)\n",
    "plt.axis('square')\n",
    "plt.plot(np.linspace(-0.001, 0.007), np.linspace(-0.001, 0.007), 'r-')\n",
    "plt.xlim(-0.001, 0.007)\n",
    "plt.ylim(-0.001, 0.007)\n",
    "plt.savefig(\"test_scatter.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101360,) (101360, 1)\n"
     ]
    }
   ],
   "source": [
    "allpred_X = np.vstack((pred_X[cho_idx,:],X1))\n",
    "print(tmaty['ext'].shape, y1.shape)\n",
    "testy     = np.append(tmaty['ext'],np.exp(y1[:,0]))\n",
    "Xp = torch.Tensor(allpred_X).cuda()\n",
    "Xp = (Xp-x_mean)/x_std\n",
    "with torch.no_grad():\n",
    "    pred_y = torch.exp(net_regr.module_(Xp)).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = 0.53 (p < 0.01)\n"
     ]
    }
   ],
   "source": [
    "from scatter_plot import scatter\n",
    "fig = plt.figure(figsize=(6.5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "ymi = np.nanmin(np.append(testy,pred_y))\n",
    "yma = np.nanmax(np.append(testy,pred_y))\n",
    "paths, slope, intercept = scatter(ax, testy, pred_y, color='b',label_p = 'lower right')\n",
    "plt.xlim(ymi, yma)\n",
    "plt.ylim(ymi, yma)\n",
    "ax.set_xlabel(r'Tmatrix cext ($\\mu$m$^{-1}$)')\n",
    "ax.set_ylabel(r'NN Predict cext ($\\mu$m$^{-1}$)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_scatter_tanh_base_oriJ_fine_n.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1) (5000, 4)\n",
      "(10000, 1) (10000, 4)\n",
      "(15000, 1) (15000, 4)\n",
      "(20000, 1) (20000, 4)\n",
      "(25000, 1) (25000, 4)\n",
      "(30000, 1) (30000, 4)\n",
      "(35000, 1) (35000, 4)\n",
      "(40000, 1) (40000, 4)\n",
      "(45000, 1) (45000, 4)\n",
      "(50000, 1) (50000, 4)\n",
      "(55000, 1) (55000, 4)\n",
      "(60000, 1) (60000, 4)\n",
      "(65000, 1) (65000, 4)\n",
      "(70000, 1) (70000, 4)\n",
      "(75000, 1) (75000, 4)\n",
      "(80000, 1) (80000, 4)\n",
      "(85000, 1) (85000, 4)\n",
      "(90000, 1) (90000, 4)\n",
      "(95000, 1) (95000, 4)\n",
      "(100000, 1) (100000, 4)\n",
      "(105000, 1) (105000, 4)\n",
      "(110000, 1) (110000, 4)\n",
      "(115000, 1) (115000, 4)\n",
      "(120000, 1) (120000, 4)\n",
      "(125000, 1) (125000, 4)\n",
      "(130000, 1) (130000, 4)\n",
      "(135000, 1) (135000, 4)\n",
      "(140000, 1) (140000, 4)\n",
      "(145000, 1) (145000, 4)\n",
      "(150000, 1) (150000, 4)\n",
      "(155000, 1) (155000, 4)\n",
      "(160000, 1) (160000, 4)\n",
      "(165000, 1) (165000, 4)\n",
      "(170000, 1) (170000, 4)\n",
      "(175000, 1) (175000, 4)\n",
      "(180000, 1) (180000, 4)\n",
      "(185000, 1) (185000, 4)\n",
      "(190000, 1) (190000, 4)\n",
      "(195000, 1) (195000, 4)\n",
      "(200000, 1) (200000, 4)\n",
      "(202720, 1) (202720, 4)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "pred_X_t = torch.tensor(allpred_X).float()\n",
    "loader = DataLoader( pred_X_t, batch_size=5000)\n",
    "all_yp = np.array([])\n",
    "all_Jp = np.array([])\n",
    "for data in loader:\n",
    "    Xp = data.cuda()\n",
    "    Xp = (Xp-x_mean)/x_std\n",
    "    pred_y, pred_J, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "    #print(pred_y.shape, pred_J.shape)\n",
    "    if len(all_yp) == 0:\n",
    "        all_yp = pred_y\n",
    "        all_Jp = pred_J\n",
    "    else:    \n",
    "        all_yp = np.vstack((all_yp, pred_y))\n",
    "        all_Jp = np.vstack((all_Jp, pred_J))\n",
    "    print(all_yp.shape, all_Jp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202720, 2)\n",
      "[-7.60090246 -6.12066931 -4.64043587 -3.16020275 -1.67996929] [1.291429   1.29571462 1.30000031 1.304286   1.3085717  1.31285739\n",
      " 1.31714296 1.32142866 1.32571435 1.349286   1.407143   1.465\n",
      " 1.522857   1.580714   1.638571   1.696429  ]\n",
      "5\n",
      "cbar_tick [-7.60090246 -6.12066931 -4.64043587 -3.16020275 -1.67996929]\n",
      "R = 0.78 (p < 0.01)\n",
      "16\n",
      "cbar_tick [1.291429   1.29571462 1.30000031 1.304286   1.3085717  1.31285739\n",
      " 1.31714296 1.32142866 1.32571435 1.349286   1.407143   1.465\n",
      " 1.522857   1.580714   1.638571   1.696429  ]\n",
      "R = 0.04 (p < 0.01)\n"
     ]
    }
   ],
   "source": [
    "Jlnk  = J1[:,0] * np.exp(X1[:,2])\n",
    "Jn    = J1[:,1]\n",
    "reJ   = np.hstack((Jlnk.reshape(-1,1), Jn.reshape(-1,1)))\n",
    "testJ = np.vstack((tmatJ['ext'].T,reJ))\n",
    "\n",
    "print(testJ.shape)\n",
    "from scatter_plot import scatter\n",
    "xx = ['lnk','n']\n",
    "xx_use = {}\n",
    "cb_t   = {}\n",
    "xx_use['lnk'] = allpred_X[:,2]\n",
    "xx_use['n']   = allpred_X[:,3]\n",
    "cb_t['lnk']   = xx_use['lnk'][:181*14*5:181*14]\n",
    "cb_t['n']     = np.sort(np.append(X1[::181*14*5,3], pred_X[cho_idx,3][::181*14*5]))\n",
    "print(cb_t['lnk'], cb_t['n'])\n",
    "for j in range(2):\n",
    "    fig = plt.figure(figsize=(6.5,4.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ymi = np.nanmin(np.append(testJ[:,j],all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j]))\n",
    "    yma = np.nanmax(np.append(testJ[:,j],all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j]))\n",
    "    print(len(cb_t[xx[j]]))\n",
    "    paths, slope, intercept = scatter(ax, testJ[:,j], all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j], \n",
    "                                      color=xx_use[xx[j]],fig=fig,cbar_label=xx[j],cbar_ticks=cb_t[xx[j]],\n",
    "                                      label_p = 'lower right')\n",
    "    plt.xlim(ymi, yma)\n",
    "    plt.ylim(ymi, yma)\n",
    "    ax.set_xlabel(r'$\\frac{\\partial{cext}}{\\partial{'+xx[j]+'}}$ from Tmatrix')\n",
    "    ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{'+xx[j]+'}}$ from NN')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"img/TMlogX_Jac\"+xx[j]+\"_scatter_tanh_base_lnJ_fine_n.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cbar_tick [-7.60090246 -6.12066931 -4.64043587 -3.16020275 -1.67996929]\n",
      "R = 0.08 (p < 0.01)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (101360,) (202720,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-0acd3856060c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"img/TMlogX_Jack_scatter_tanh_fine_n_0.1scale.png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtJ\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtmatJ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ext'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mpJ\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mall_Jp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtJ\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0088\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtJ\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0089\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (101360,) (202720,) "
     ]
    }
   ],
   "source": [
    "from scatter_plot import scatter\n",
    "kk  = np.exp(allpred_X[:,2])\n",
    "kname = ['{:.4f}'.format(ik) for ik in kk[:181*14*5:181*14]]\n",
    "fig = plt.figure(figsize=(6.5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "j = 0\n",
    "ymi = np.nanmin(np.append(testJ[:,j]/kk,all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j]/kk))\n",
    "yma = np.nanmax(np.append(testJ[:,j]/kk,all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j]/kk))\n",
    "paths, slope, intercept = scatter(ax, testJ[:,j]/kk, all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j]/kk, \n",
    "                                  color=xx_use[xx[j]],fig=fig,cbar_label='k',cbar_ticks=cb_t[xx[j]],\n",
    "                                  cticklabel=kname, label_p = 'lower right')\n",
    "plt.xlim(ymi, yma)\n",
    "plt.ylim(ymi, yma)\n",
    "ax.set_xlabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$ from Tmatrix')\n",
    "ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$ from NN')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_Jack_scatter_tanh_fine_n_0.1scale.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()\n",
    "tJ  = tmatJ['ext'][j,:]/kk\n",
    "pJ  = all_Jp[:,2+j]/x_std.cpu().numpy()[0][2+j]/kk\n",
    "idx = np.where((tJ > 0.0088) & (tJ < 0.0089))\n",
    "print(tJ[idx], pJ[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numk = 5\n",
    "kk  = np.exp(pred_X[cho_idx,2])\n",
    "for i in range(numk):\n",
    "    fig = plt.figure(figsize=(5.5,4.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    j = 0\n",
    "    idx2 = np.arange(181*14*i,181*14*(i+1))    \n",
    "    ymi = np.nanmin(np.append(tmatJ['ext'][j,idx2]/kk[idx2],all_Jp[idx2,2+j]/x_std.cpu().numpy()[0][2+j]/kk[idx2]))\n",
    "    yma = np.nanmax(np.append(tmatJ['ext'][j,idx2]/kk[idx2],all_Jp[idx2,2+j]/x_std.cpu().numpy()[0][2+j]/kk[idx2]))\n",
    "    paths, slope, intercept = scatter(ax, tmatJ['ext'][j,idx2]/kk[idx2], all_Jp[idx2,2+j]/x_std.cpu().numpy()[0][2+j]/kk[idx2], \n",
    "                                  color='b',label_p = 'lower right')\n",
    "    ax.set_title('k = {:.4f}'.format(kk[181*14*i]))\n",
    "    plt.xlim(ymi, yma)\n",
    "    plt.ylim(ymi, yma)\n",
    "    ax.set_xlabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$ from Tmatrix')\n",
    "    ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$ from NN')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"img/TMlogX_Jack_scatter_tanh0.01reg_k\"+str(i)+\".png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4\n",
    "inputx = pred_X[cho_idx[(n-1)*181*14:(n-1)*181*14+181],:]\n",
    "Xp = torch.Tensor(inputx).cuda()\n",
    "Xp = (Xp-x_mean)/x_std\n",
    "pred_y, pred_J, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "fig = plt.figure(figsize=(6.5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "tJ = tmatJ['ext'][0,(n-1)*181*14:(n-1)*181*14+181] / np.exp(inputx[:,2])\n",
    "ax.scatter(inputx[:,0], tJ, label='Tmatrix' )\n",
    "ax.scatter(inputx[:,0], pred_J[:,2]/x_std.cpu().numpy()[0][2]/np.exp(inputx[:,2]), label='NN' )\n",
    "ax.set_xlabel('angle')\n",
    "ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_Jack_tanh0.01reg_angle.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()\n",
    "print(tJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scatter_plot import scatter\n",
    "yp = []\n",
    "xx = ['k','n']\n",
    "aveJ = {}\n",
    "aveJ['k'] = []\n",
    "aveJ['n'] = []\n",
    "for n in range(14*5*8):\n",
    "    inputx = pred_X[cho_idx[n*181:(n+1)*181],:]\n",
    "    Xp = torch.Tensor(inputx).cuda()\n",
    "    Xp = (Xp-x_mean)/x_std\n",
    "    pred_y, pred_J, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "    yp.append( np.nanmean(pred_y) )\n",
    "    Jack = pred_J[:,2] / x_std.cpu().numpy()[0][2]/ np.exp(inputx[:,2])\n",
    "    Jacn = pred_J[:,3] / x_std.cpu().numpy()[0][3]\n",
    "    aveJ['k'].append( np.nanmean(Jack) )\n",
    "    aveJ['n'].append( np.nanmean(Jacn) )\n",
    "\n",
    "xx_use = {}\n",
    "cb_t   = {}\n",
    "xname  = {}\n",
    "xx_use['k'] = pred_X[cho_idx[::181],2]\n",
    "xx_use['n'] = pred_X[cho_idx[::181],3]\n",
    "cb_t['k']   = xx_use['k'][:14*5:14]\n",
    "cb_t['n']   = xx_use['n'][::14*5]\n",
    "xname['k']  = ['{:.4f}'.format(np.exp(ik)) for ik in cb_t['k']]\n",
    "xname['n']  = ['{:.4f}'.format(inn) for inn in cb_t['n']]\n",
    "for j in range(len(xx)):\n",
    "    ix = xx[j]\n",
    "    fig = plt.figure(figsize=(6.5,4.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    if j == 0:\n",
    "        tJ = tmatJ['ext'][j,::181] / np.exp(pred_X[cho_idx[::181],2])\n",
    "    else:\n",
    "        tJ = tmatJ['ext'][j,::181]\n",
    "    ymi = np.nanmin(np.append(tJ,np.array(aveJ[ix])))\n",
    "    yma = np.nanmax(np.append(tJ,np.array(aveJ[ix])))\n",
    "    paths, slope, intercept = scatter(ax, tJ, np.array(aveJ[ix]), \n",
    "                                  color=xx_use[ix],fig=fig,cbar_label='k',cbar_ticks=cb_t[ix],\n",
    "                                  cticklabel=xname[ix], label_p = 'lower right')\n",
    "    plt.xlim(ymi, yma)\n",
    "    plt.ylim(ymi, yma)\n",
    "    ax.set_xlabel(r'$\\frac{\\partial{cext}}{\\partial{'+ix+'}}$ from Tmatrix')\n",
    "    ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{'+ix+'}}$ from NN')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"img/TMlogX_Jac\"+ix+\"_ave_tanh0.01reg_scatter.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 181,14,5,8\n",
    "n = 1\n",
    "j = 0\n",
    "useX = pred_X[cho_idx,:]\n",
    "inputx = useX[n::181,:]\n",
    "Xp = torch.Tensor(inputx).cuda()\n",
    "Xp = (Xp-x_mean)/x_std\n",
    "pred_y, pred_J, xt = get_ut_jacobian(net_regr.module_, Xp)\n",
    "\n",
    "fig = plt.figure(figsize=(5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "tJ = tmatJ['ext'][j,n::181] / np.exp(inputx[:,2])\n",
    "ax.scatter(np.exp(inputx[:14*5:14,2]), tJ[:14*5:14], label='Tmatrix' )\n",
    "ax.scatter(np.exp(inputx[:14*5:14,2]), pred_J[:14*5:14,2]/x_std.cpu().numpy()[0][2]/np.exp(inputx[:14*5:14,2]), label='NN' )\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{k}}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_Jack_k_tanh0.01reg.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()\n",
    "\n",
    "fig = plt.figure(figsize=(5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "tmy = tmaty['ext'][n::181] \n",
    "ax.plot(np.exp(inputx[:14*5:14,2]), tmy[:14*5:14], 'o-', label='Tmatrix' )\n",
    "ax.plot(np.exp(inputx[:14*5:14,2]), pred_y[:14*5:14], 'o-', label='NN' )\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cext')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_ext_k_tanh0.01reg.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()\n",
    "\n",
    "fig = plt.figure(figsize=(5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(inputx[::14*5,3], tmy[::14*5], 'o-', label='Tmatrix' )\n",
    "ax.plot(inputx[::14*5,3], pred_y[::14*5], 'o-', label='NN' )\n",
    "ax.set_xlabel('n')\n",
    "ax.set_ylabel('cext')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_ext_n_tanh0.01reg.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()\n",
    "\n",
    "fig = plt.figure(figsize=(5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.exp(inputx[:14,1]), tmy[:14], 'o-', label='Tmatrix' )\n",
    "ax.plot(np.exp(inputx[:14,1]), pred_y[:14], 'o-', label='NN' )\n",
    "ax.set_xlabel('r')\n",
    "ax.set_ylabel('cext')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_ext_r_tanh0.01reg.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,4.5))\n",
    "ax = fig.add_subplot(111)\n",
    "j  = 1\n",
    "tJ = tmatJ['ext'][j,n::181]\n",
    "ax.scatter(inputx[::14*5,3], tJ[::14*5], label='Tmatrix' )\n",
    "ax.scatter(inputx[::14*5,3], pred_J[::14*5,3]/x_std.cpu().numpy()[0][3], label='NN' )\n",
    "ax.set_xlabel('n')\n",
    "ax.set_ylabel(r'$\\frac{\\partial{cext}}{\\partial{n}}$')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/TMlogX_Jacn_n_tanh0.01reg.png\", dpi=300, facecolor='w', edgecolor='w')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "[[12 14 16 18]\n",
      " [20 22 24 26]\n",
      " [28 30 32 34]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.arange(24).reshape((2,3,4))\n",
    "print(a)\n",
    "print(np.sum(a, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
